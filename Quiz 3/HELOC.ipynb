{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load FICO HELOC data with special values converted to np.nan\n",
    "from aix360.datasets.heloc_dataset import HELOCDataset, nan_preprocessing\n",
    "data = HELOCDataset(custom_preprocessing=nan_preprocessing).data()\n",
    "# Separate target variable\n",
    "y = data.pop('RiskPerformance')\n",
    "\n",
    "# Split data into training and test sets using fixed random seed\n",
    "from sklearn.model_selection import train_test_split\n",
    "dfTrain, dfTest, yTrain, yTest = train_test_split(data, y, random_state=0, stratify=y)\n",
    "dfTrain.head().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>operation</th>\n",
       "      <th colspan=\"9\" halign=\"left\">&lt;=</th>\n",
       "      <th colspan=\"9\" halign=\"left\">&gt;</th>\n",
       "      <th>==</th>\n",
       "      <th>!=</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>value</th>\n",
       "      <th>59.0</th>\n",
       "      <th>63.0</th>\n",
       "      <th>66.0</th>\n",
       "      <th>69.0</th>\n",
       "      <th>72.0</th>\n",
       "      <th>75.0</th>\n",
       "      <th>78.0</th>\n",
       "      <th>82.0</th>\n",
       "      <th>86.0</th>\n",
       "      <th>59.0</th>\n",
       "      <th>63.0</th>\n",
       "      <th>66.0</th>\n",
       "      <th>69.0</th>\n",
       "      <th>72.0</th>\n",
       "      <th>75.0</th>\n",
       "      <th>78.0</th>\n",
       "      <th>82.0</th>\n",
       "      <th>86.0</th>\n",
       "      <th>NaN</th>\n",
       "      <th>NaN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8960</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8403</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1949</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4886</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "operation   <=                                            >                 \\\n",
       "value     59.0 63.0 66.0 69.0 72.0 75.0 78.0 82.0 86.0 59.0 63.0 66.0 69.0   \n",
       "8960         0    0    1    1    1    1    1    1    1    1    1    0    0   \n",
       "8403         1    1    1    1    1    1    1    1    1    0    0    0    0   \n",
       "1949         1    1    1    1    1    1    1    1    1    0    0    0    0   \n",
       "4886         0    0    1    1    1    1    1    1    1    1    1    0    0   \n",
       "4998         0    0    1    1    1    1    1    1    1    1    1    0    0   \n",
       "\n",
       "operation                           ==  !=  \n",
       "value     72.0 75.0 78.0 82.0 86.0 NaN NaN  \n",
       "8960         0    0    0    0    0   0   1  \n",
       "8403         0    0    0    0    0   0   1  \n",
       "1949         0    0    0    0    0   0   1  \n",
       "4886         0    0    0    0    0   0   1  \n",
       "4998         0    0    0    0    0   0   1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binarize data and also return standardized ordinal features\n",
    "from aix360.algorithms.rbm import FeatureBinarizer\n",
    "fb = FeatureBinarizer(negations=True, returnOrd=True)\n",
    "dfTrain, dfTrainStd = fb.fit_transform(dfTrain)\n",
    "dfTest, dfTestStd = fb.transform(dfTest)\n",
    "dfTrain['ExternalRiskEstimate'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import statements\n",
    "\n",
    "Import necessary libraries, frameworks and algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model, load_model, model_from_json\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "from aix360.algorithms.contrastive import CEMExplainer, KerasClassifier\n",
    "from aix360.algorithms.protodash import ProtodashExplainer\n",
    "from aix360.datasets.heloc_dataset import HELOCDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load HELOC dataset and show sample applicants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Heloc dataset:  c:\\users\\ronnyluss\\aix360\\aix360\\datasets\\..\\data\\heloc_data\\heloc_dataset.csv\n",
      "Size of HELOC dataset: (10459, 24)\n",
      "Number of \"Good\" applicants: 5000\n",
      "Number of \"Bad\" applicants: 5459\n",
      "Sample Applicants:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ExternalRiskEstimate</th>\n",
       "      <td>55</td>\n",
       "      <td>61</td>\n",
       "      <td>67</td>\n",
       "      <td>66</td>\n",
       "      <td>81</td>\n",
       "      <td>59</td>\n",
       "      <td>54</td>\n",
       "      <td>68</td>\n",
       "      <td>59</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSinceOldestTradeOpen</th>\n",
       "      <td>144</td>\n",
       "      <td>58</td>\n",
       "      <td>66</td>\n",
       "      <td>169</td>\n",
       "      <td>333</td>\n",
       "      <td>137</td>\n",
       "      <td>88</td>\n",
       "      <td>148</td>\n",
       "      <td>324</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSinceMostRecentTradeOpen</th>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AverageMInFile</th>\n",
       "      <td>84</td>\n",
       "      <td>41</td>\n",
       "      <td>24</td>\n",
       "      <td>73</td>\n",
       "      <td>132</td>\n",
       "      <td>78</td>\n",
       "      <td>37</td>\n",
       "      <td>65</td>\n",
       "      <td>138</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumSatisfactoryTrades</th>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>28</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>25</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumTrades60Ever2DerogPubRec</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumTrades90Ever2DerogPubRec</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PercentTradesNeverDelq</th>\n",
       "      <td>83</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>93</td>\n",
       "      <td>100</td>\n",
       "      <td>91</td>\n",
       "      <td>92</td>\n",
       "      <td>83</td>\n",
       "      <td>85</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSinceMostRecentDelq</th>\n",
       "      <td>2</td>\n",
       "      <td>-7</td>\n",
       "      <td>-7</td>\n",
       "      <td>76</td>\n",
       "      <td>-7</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>31</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MaxDelq2PublicRecLast12M</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MaxDelqEver</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumTotalTrades</th>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>32</td>\n",
       "      <td>26</td>\n",
       "      <td>18</td>\n",
       "      <td>27</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumTradesOpeninLast12M</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PercentInstallTrades</th>\n",
       "      <td>43</td>\n",
       "      <td>67</td>\n",
       "      <td>44</td>\n",
       "      <td>57</td>\n",
       "      <td>25</td>\n",
       "      <td>47</td>\n",
       "      <td>58</td>\n",
       "      <td>44</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSinceMostRecentInqexcl7days</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumInqLast6M</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumInqLast6Mexcl7days</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NetFractionRevolvingBurden</th>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>72</td>\n",
       "      <td>51</td>\n",
       "      <td>62</td>\n",
       "      <td>89</td>\n",
       "      <td>28</td>\n",
       "      <td>68</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NetFractionInstallBurden</th>\n",
       "      <td>-8</td>\n",
       "      <td>-8</td>\n",
       "      <td>66</td>\n",
       "      <td>83</td>\n",
       "      <td>89</td>\n",
       "      <td>93</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>-8</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumRevolvingTradesWBalance</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumInstallTradesWBalance</th>\n",
       "      <td>1</td>\n",
       "      <td>-8</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NumBank2NatlTradesWHighUtilization</th>\n",
       "      <td>1</td>\n",
       "      <td>-8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PercentTradesWBalance</th>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>91</td>\n",
       "      <td>80</td>\n",
       "      <td>94</td>\n",
       "      <td>100</td>\n",
       "      <td>40</td>\n",
       "      <td>90</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RiskPerformance</th>\n",
       "      <td>Bad</td>\n",
       "      <td>Bad</td>\n",
       "      <td>Bad</td>\n",
       "      <td>Bad</td>\n",
       "      <td>Bad</td>\n",
       "      <td>Bad</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>Bad</td>\n",
       "      <td>Bad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      0    1    2    3    4    5     6     7    8    9\n",
       "ExternalRiskEstimate                 55   61   67   66   81   59    54    68   59   61\n",
       "MSinceOldestTradeOpen               144   58   66  169  333  137    88   148  324   79\n",
       "MSinceMostRecentTradeOpen             4   15    5    1   27   11     7     7    2    4\n",
       "AverageMInFile                       84   41   24   73  132   78    37    65  138   36\n",
       "NumSatisfactoryTrades                20    2    9   28   12   31    25    17   24   19\n",
       "NumTrades60Ever2DerogPubRec           3    4    0    1    0    0     0     0    0    0\n",
       "NumTrades90Ever2DerogPubRec           0    4    0    1    0    0     0     0    0    0\n",
       "PercentTradesNeverDelq               83  100  100   93  100   91    92    83   85   95\n",
       "MSinceMostRecentDelq                  2   -7   -7   76   -7    1     9    31    5    5\n",
       "MaxDelq2PublicRecLast12M              3    0    7    6    7    4     4     6    4    4\n",
       "MaxDelqEver                           5    8    8    6    8    6     6     6    6    6\n",
       "NumTotalTrades                       23    7    9   30   12   32    26    18   27   19\n",
       "NumTradesOpeninLast12M                1    0    4    3    0    1     3     1    1    3\n",
       "PercentInstallTrades                 43   67   44   57   25   47    58    44   26   26\n",
       "MSinceMostRecentInqexcl7days          0    0    0    0    0    0     0     0    0    0\n",
       "NumInqLast6M                          0    0    4    5    1    0     4     0    1    6\n",
       "NumInqLast6Mexcl7days                 0    0    4    4    1    0     4     0    1    6\n",
       "NetFractionRevolvingBurden           33    0   53   72   51   62    89    28   68   31\n",
       "NetFractionInstallBurden             -8   -8   66   83   89   93    76    48   -8   86\n",
       "NumRevolvingTradesWBalance            8    0    4    6    3   12     7     2    7    5\n",
       "NumInstallTradesWBalance              1   -8    2    4    1    4     7     2    1    3\n",
       "NumBank2NatlTradesWHighUtilization    1   -8    1    3    0    3     2     2    3    1\n",
       "PercentTradesWBalance                69    0   86   91   80   94   100    40   90   62\n",
       "RiskPerformance                     Bad  Bad  Bad  Bad  Bad  Bad  Good  Good  Bad  Bad"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heloc = HELOCDataset()\n",
    "df = heloc.dataframe()\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 24)\n",
    "pd.set_option('display.width', 1000)\n",
    "print(\"Size of HELOC dataset:\", df.shape)\n",
    "print(\"Number of \\\"Good\\\" applicants:\", np.sum(df['RiskPerformance']=='Good'))\n",
    "print(\"Number of \\\"Bad\\\" applicants:\", np.sum(df['RiskPerformance']=='Bad'))\n",
    "print(\"Sample Applicants:\")\n",
    "df.head(10).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of ExternalRiskEstimate and NumSatisfactoryTrades columns:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot (example) distributions for two features\n",
    "print(\"Distribution of ExternalRiskEstimate and NumSatisfactoryTrades columns:\")\n",
    "hist = df.hist(column=['ExternalRiskEstimate', 'NumSatisfactoryTrades'], bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Process the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data and split dataset into train/test\n",
    "(Data, x_train, x_test, y_train_b, y_test_b) = heloc.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "Z = np.vstack((x_train, x_test))\n",
    "Zmax = np.max(Z, axis=0)\n",
    "Zmin = np.min(Z, axis=0)\n",
    "\n",
    "#normalize an array of samples to range [-0.5, 0.5]\n",
    "def normalize(V):\n",
    "    VN = (V - Zmin)/(Zmax - Zmin)\n",
    "    VN = VN - 0.5\n",
    "    return(VN)\n",
    "    \n",
    "# rescale a sample to recover original values for normalized values. \n",
    "def rescale(X):\n",
    "    return(np.multiply ( X + 0.5, (Zmax - Zmin) ) + Zmin)\n",
    "\n",
    "N = normalize(Z)\n",
    "xn_train = N[0:x_train.shape[0], :]\n",
    "xn_test  = N[x_train.shape[0]:, :]\n",
    "def nn_small():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=23, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(2, kernel_initializer='normal'))    \n",
    "    return model  \n",
    "# Set random seeds for repeatability\n",
    "np.random.seed(1) \n",
    "tf.set_random_seed(2) \n",
    "\n",
    "class_names = ['Bad', 'Good']\n",
    "\n",
    "# loss function\n",
    "def fn(correct, predicted):\n",
    "    return tf.nn.softmax_cross_entropy_with_logits(labels=correct, logits=predicted)\n",
    "\n",
    "# compile and print model summary\n",
    "nn = nn_small()\n",
    "nn.compile(loss=fn, optimizer='adam', metrics=['accuracy'])\n",
    "nn.summary()\n",
    "\n",
    "\n",
    "# train model or load a trained model\n",
    "TRAIN_MODEL = True\n",
    "\n",
    "if (TRAIN_MODEL): \n",
    "    nn.fit(xn_train, y_train_b, batch_size=128, epochs=500, verbose=1, shuffle=False)\n",
    "    nn.save_weights(\"heloc_nnsmall.h5\")     \n",
    "else:    \n",
    "    nn.load_weights(\"heloc_nnsmall.h5\")\n",
    "        \n",
    "\n",
    "# evaluate model accuracy        \n",
    "score = nn.evaluate(xn_train, y_train_b, verbose=0) #Compute training set accuracy\n",
    "print('Train accuracy:', score[1])\n",
    "\n",
    "score = nn.evaluate(xn_test, y_test_b, verbose=0) #Compute test set accuracy\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "columns = list(dfTrain.columns)\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(xn_train, feature_names=columns, class_names=['RiskPerformance'], discretize_continuous=True)\n",
    "exp = explainer.explain_instance(xn_test[1], nn.predict_proba, top_labels=1)\n",
    "\n",
    "# Show explanation\n",
    "exp.show_in_notebook(show_table=True, show_all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_train = nn.predict_classes(xn_train) # Use trained neural network to predict train points\n",
    "p_train = p_train.reshape((p_train.shape[0],1))\n",
    "\n",
    "z_train = np.hstack((xn_train, p_train)) # Store (normalized) instances that were predicted as Good\n",
    "z_train_good = z_train[z_train[:,-1]==1, :]\n",
    "\n",
    "zun_train = np.hstack((x_train, p_train)) # Store (unnormalized) instances that were predicted as Good \n",
    "zun_train_good = zun_train[zun_train[:,-1]==1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 3 \n",
    "\n",
    "X = xn_test[idx].reshape((1,) + xn_test[idx].shape)\n",
    "\n",
    "print(\"Chosen Sample:\", idx)\n",
    "print(\"Prediction made by the model:\", class_names[np.argmax(nn.predict_proba(X))])\n",
    "print(\"Prediction probabilities:\", nn.predict_proba(X))\n",
    "print(\"\")\n",
    "\n",
    "# attach the prediction made by the model to X\n",
    "X = np.hstack((X, nn.predict_classes(X).reshape((1,1))))\n",
    "\n",
    "Xun = x_test[idx].reshape((1,) + x_test[idx].shape) \n",
    "dfx = pd.DataFrame.from_records(Xun.astype('double')) # Create dataframe with original feature values\n",
    "dfx[23] = class_names[int(X[0, -1])]\n",
    "dfx.columns = df.columns\n",
    "dfx.transpose()\n",
    "\n",
    "explainer = ProtodashExplainer()\n",
    "(W, S, setValues) = explainer.explain(X, z_train_good, m=5) # Return weights W, Prototypes S and objective function values\n",
    "\n",
    "dfs = pd.DataFrame.from_records(zun_train_good[S, 0:-1].astype('double'))\n",
    "RP=[]\n",
    "for i in range(S.shape[0]):\n",
    "    RP.append(class_names[int(z_train_good[S[i], -1])]) # Append class names\n",
    "dfs[23] = RP\n",
    "dfs.columns = df.columns  \n",
    "dfs[\"Weight\"] = np.around(W, 5)/np.sum(np.around(W, 5)) # Calculate normalized importance weights\n",
    "dfs.transpose()\n",
    "\n",
    "z = z_train_good[S, 0:-1] # Store chosen prototypes\n",
    "eps = 1e-10 # Small constant defined to eliminate divide-by-zero errors\n",
    "fwt = np.zeros(z.shape)\n",
    "for i in range (z.shape[0]):\n",
    "    for j in range(z.shape[1]):\n",
    "        fwt[i, j] = np.exp(-1 * abs(X[0, j] - z[i,j])/(np.std(z[:, j])+eps)) # Compute feature similarity in [0,1]\n",
    "                \n",
    "# move wts to a dataframe to display\n",
    "dfw = pd.DataFrame.from_records(np.around(fwt.astype('double'), 2))\n",
    "dfw.columns = df.columns[:-1]\n",
    "dfw.transpose()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "heloc = HELOCDataset()\n",
    "df = heloc.dataframe()\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 24)\n",
    "pd.set_option('display.width', 1000)\n",
    "print(\"Size of HELOC dataset:\", df.shape)\n",
    "print(\"Number of \\\"Good\\\" applicants:\", np.sum(df['RiskPerformance']=='Good'))\n",
    "print(\"Number of \\\"Bad\\\" applicants:\", np.sum(df['RiskPerformance']=='Bad'))\n",
    "print(\"Sample Applicants:\")\n",
    "df.head(10).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data and split dataset into train/test\n",
    "PROCESS_DATA = False\n",
    "\n",
    "if (PROCESS_DATA): \n",
    "    (Data, x_train, x_test, y_train_b, y_test_b) = heloc.split()\n",
    "    np.savez('heloc.npz', Data=Data, x_train=x_train, x_test=x_test, y_train_b=y_train_b, y_test_b=y_test_b)\n",
    "else:\n",
    "    heloc = np.load('heloc.npz', allow_pickle = True)\n",
    "    Data = heloc['Data']\n",
    "    x_train = heloc['x_train']\n",
    "    x_test  = heloc['x_test']\n",
    "    y_train_b = heloc['y_train_b']\n",
    "    y_test_b  = heloc['y_test_b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = np.vstack((x_train, x_test))\n",
    "Zmax = np.max(Z, axis=0)\n",
    "Zmin = np.min(Z, axis=0)\n",
    "\n",
    "#normalize an array of samples to range [-0.5, 0.5]\n",
    "def normalize(V):\n",
    "    VN = (V - Zmin)/(Zmax - Zmin)\n",
    "    VN = VN - 0.5\n",
    "    return(VN)\n",
    "    \n",
    "# rescale a sample to recover original values for normalized values. \n",
    "def rescale(X):\n",
    "    return(np.multiply ( X + 0.5, (Zmax - Zmin) ) + Zmin)\n",
    "\n",
    "N = normalize(Z)\n",
    "xn_train = N[0:x_train.shape[0], :]\n",
    "xn_test  = N[x_train.shape[0]:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn with no softmax\n",
    "def nn_small():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=23, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(2, kernel_initializer='normal'))    \n",
    "    return model    \n",
    "\n",
    "# Set random seeds for repeatability\n",
    "np.random.seed(1) \n",
    "tf.set_random_seed(2) \n",
    "\n",
    "class_names = ['Bad', 'Good']\n",
    "\n",
    "# loss function\n",
    "def fn(correct, predicted):\n",
    "    return tf.nn.softmax_cross_entropy_with_logits(labels=correct, logits=predicted)\n",
    "\n",
    "# compile and print model summary\n",
    "nn = nn_small()\n",
    "nn.compile(loss=fn, optimizer='adam', metrics=['accuracy'])\n",
    "nn.summary()\n",
    "\n",
    "\n",
    "# train model or load a trained model\n",
    "TRAIN_MODEL = False\n",
    "\n",
    "if (TRAIN_MODEL):             \n",
    "    nn.fit(xn_train, y_train_b, batch_size=128, epochs=500, verbose=1, shuffle=False)\n",
    "    nn.save_weights(\"heloc_nnsmall.h5\")     \n",
    "else:    \n",
    "    nn.load_weights(\"heloc_nnsmall.h5\")\n",
    "        \n",
    "\n",
    "# evaluate model accuracy        \n",
    "score = nn.evaluate(xn_train, y_train_b, verbose=0) #Compute training set accuracy\n",
    "#print('Train loss:', score[0])\n",
    "print('Train accuracy:', score[1])\n",
    "\n",
    "score = nn.evaluate(xn_test, y_test_b, verbose=0) #Compute test set accuracy\n",
    "#print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2344\n",
    "\n",
    "X = xn_test[idx].reshape((1,) + xn_test[idx].shape)\n",
    "print(\"Computing PN for Sample:\", idx)\n",
    "print(\"Prediction made by the model:\", nn.predict_proba(X))\n",
    "print(\"Prediction probabilities:\", class_names[np.argmax(nn.predict_proba(X))])\n",
    "print(\"\")\n",
    "\n",
    "mymodel = KerasClassifier(nn)\n",
    "explainer = CEMExplainer(mymodel)\n",
    "\n",
    "arg_mode = 'PN' # Find pertinent negatives\n",
    "arg_max_iter = 1000 # Maximum number of iterations to search for the optimal PN for given parameter settings\n",
    "arg_init_const = 10.0 # Initial coefficient value for main loss term that encourages class change\n",
    "arg_b = 9 # No. of updates to the coefficient of the main loss term\n",
    "arg_kappa = 0.2 # Minimum confidence gap between the PNs (changed) class probability and original class' probability\n",
    "arg_beta = 1e-1 # Controls sparsity of the solution (L1 loss)\n",
    "arg_gamma = 100 # Controls how much to adhere to a (optionally trained) auto-encoder\n",
    "my_AE_model = None # Pointer to an auto-encoder\n",
    "arg_alpha = 0.01 # Penalizes L2 norm of the solution\n",
    "arg_threshold = 1. # Automatically turn off features <= arg_threshold if arg_threshold < 1\n",
    "arg_offset = 0.5 # the model assumes classifier trained on data normalized\n",
    "                # in [-arg_offset, arg_offset] range, where arg_offset is 0 or 0.5\n",
    "# Find PN for applicant 1272\n",
    "(adv_pn, delta_pn, info_pn) = explainer.explain_instance(X, arg_mode, my_AE_model, arg_kappa, arg_b,\n",
    "                                                         arg_max_iter, arg_init_const, arg_beta, arg_gamma,\n",
    "                                                            arg_alpha, arg_threshold, arg_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xpn = adv_pn\n",
    "classes = [ class_names[np.argmax(nn.predict_proba(X))], class_names[np.argmax(nn.predict_proba(Xpn))], 'NIL' ]\n",
    "\n",
    "print(\"Sample:\", idx)\n",
    "print(\"prediction(X)\", nn.predict_proba(X), class_names[np.argmax(nn.predict_proba(X))])\n",
    "print(\"prediction(Xpn)\", nn.predict_proba(Xpn), class_names[np.argmax(nn.predict_proba(Xpn))] )\n",
    "\n",
    "\n",
    "X_re = rescale(X) # Convert values back to original scale from normalized\n",
    "Xpn_re = rescale(Xpn)\n",
    "Xpn_re = np.around(Xpn_re.astype(np.double), 2)\n",
    "\n",
    "delta_re = Xpn_re - X_re\n",
    "delta_re = np.around(delta_re.astype(np.double), 2)\n",
    "delta_re[np.absolute(delta_re) < 1e-4] = 0\n",
    "\n",
    "X3 = np.vstack((X_re, Xpn_re, delta_re))\n",
    "\n",
    "dfre = pd.DataFrame.from_records(X3) # Create dataframe to display original point, PN and difference (delta)\n",
    "dfre[23] = classes\n",
    "\n",
    "dfre.columns = df.columns\n",
    "dfre.rename(index={0:'X',1:'X_PN', 2:'(X_PN - X)'}, inplace=True)\n",
    "dfret = dfre.transpose()\n",
    "\n",
    "\n",
    "def highlight_ce(s, col, ncols):\n",
    "    if (type(s[col]) != str):\n",
    "        if (s[col] > 0):\n",
    "            return(['background-color: yellow']*ncols)    \n",
    "    return(['background-color: white']*ncols)\n",
    "\n",
    "dfret.style.apply(highlight_ce, col='(X_PN - X)', ncols=3, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fi = abs((X-Xpn).astype('double'))/np.std(xn_train.astype('double'), axis=0) # Compute PN feature importance\n",
    "objects = df.columns[-2::-1]\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = fi[0, -1::-1]\n",
    "\n",
    "plt.barh(y_pos, performance, align='center', alpha=0.5) # bar chart\n",
    "plt.yticks(y_pos, objects) # Display features on y-axis\n",
    "plt.xlabel('weight') # x-label\n",
    "plt.title('PN (feature importance)') # Heading\n",
    "\n",
    "plt.show() # Display PN feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 449\n",
    "\n",
    "X = xn_test[idx].reshape((1,) + xn_test[idx].shape)\n",
    "print(\"Computing PN for Sample:\", idx)\n",
    "print(\"Prediction made by the model:\", nn.predict_proba(X))\n",
    "print(\"Prediction probabilities:\", class_names[np.argmax(nn.predict_proba(X))])\n",
    "print(\"\")\n",
    "\n",
    "mymodel = KerasClassifier(nn)\n",
    "explainer = CEMExplainer(mymodel)\n",
    "\n",
    "arg_mode = 'PN' # Find pertinent negatives\n",
    "arg_max_iter = 1000 # Maximum number of iterations to search for the optimal PN for given parameter settings\n",
    "arg_init_const = 10.0 # Initial coefficient value for main loss term that encourages class change\n",
    "arg_b = 9 # No. of updates to the coefficient of the main loss term\n",
    "arg_kappa = 0.2 # Minimum confidence gap between the PNs (changed) class probability and original class' probability\n",
    "arg_beta = 1e-1 # Controls sparsity of the solution (L1 loss)\n",
    "arg_gamma = 100 # Controls how much to adhere to a (optionally trained) auto-encoder\n",
    "my_AE_model = None # Pointer to an auto-encoder\n",
    "arg_alpha = 0.01 # Penalizes L2 norm of the solution\n",
    "arg_threshold = 1. # Automatically turn off features <= arg_threshold if arg_threshold < 1\n",
    "arg_offset = 0.5 # the model assumes classifier trained on data normalized\n",
    "                # in [-arg_offset, arg_offset] range, where arg_offset is 0 or 0.5\n",
    "# Find PN for applicant 1272\n",
    "(adv_pn, delta_pn, info_pn) = explainer.explain_instance(X, arg_mode, my_AE_model, arg_kappa, arg_b,\n",
    "                                                         arg_max_iter, arg_init_const, arg_beta, arg_gamma,\n",
    "                                                            arg_alpha, arg_threshold, arg_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xpn = adv_pn\n",
    "classes = [ class_names[np.argmax(nn.predict_proba(X))], class_names[np.argmax(nn.predict_proba(Xpn))], 'NIL' ]\n",
    "\n",
    "print(\"Sample:\", idx)\n",
    "print(\"prediction(X)\", nn.predict_proba(X), class_names[np.argmax(nn.predict_proba(X))])\n",
    "print(\"prediction(Xpn)\", nn.predict_proba(Xpn), class_names[np.argmax(nn.predict_proba(Xpn))] )\n",
    "\n",
    "\n",
    "X_re = rescale(X) # Convert values back to original scale from normalized\n",
    "Xpn_re = rescale(Xpn)\n",
    "Xpn_re = np.around(Xpn_re.astype(np.double), 2)\n",
    "\n",
    "delta_re = Xpn_re - X_re\n",
    "delta_re = np.around(delta_re.astype(np.double), 2)\n",
    "delta_re[np.absolute(delta_re) < 1e-4] = 0\n",
    "\n",
    "X3 = np.vstack((X_re, Xpn_re, delta_re))\n",
    "\n",
    "dfre = pd.DataFrame.from_records(X3) # Create dataframe to display original point, PN and difference (delta)\n",
    "dfre[23] = classes\n",
    "\n",
    "dfre.columns = df.columns\n",
    "dfre.rename(index={0:'X',1:'X_PN', 2:'(X_PN - X)'}, inplace=True)\n",
    "dfret = dfre.transpose()\n",
    "\n",
    "\n",
    "def highlight_ce(s, col, ncols):\n",
    "    if (type(s[col]) != str):\n",
    "        if (s[col] > 0):\n",
    "            return(['background-color: yellow']*ncols)    \n",
    "    return(['background-color: white']*ncols)\n",
    "\n",
    "dfret.style.apply(highlight_ce, col='(X_PN - X)', ncols=3, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fi = abs((X-Xpn).astype('double'))/np.std(xn_train.astype('double'), axis=0) # Compute PN feature importance\n",
    "objects = df.columns[-2::-1]\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = fi[0, -1::-1]\n",
    "\n",
    "plt.barh(y_pos, performance, align='center', alpha=0.5) # bar chart\n",
    "plt.yticks(y_pos, objects) # Display features on y-axis\n",
    "plt.xlabel('weight') # x-label\n",
    "plt.title('PN (feature importance)') # Heading\n",
    "\n",
    "plt.show() # Display PN feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1168\n",
    "\n",
    "X = xn_test[idx].reshape((1,) + xn_test[idx].shape)\n",
    "print(\"Computing PN for Sample:\", idx)\n",
    "print(\"Prediction made by the model:\", nn.predict_proba(X))\n",
    "print(\"Prediction probabilities:\", class_names[np.argmax(nn.predict_proba(X))])\n",
    "print(\"\")\n",
    "\n",
    "mymodel = KerasClassifier(nn)\n",
    "explainer = CEMExplainer(mymodel)\n",
    "\n",
    "arg_mode = 'PN' # Find pertinent negatives\n",
    "arg_max_iter = 1000 # Maximum number of iterations to search for the optimal PN for given parameter settings\n",
    "arg_init_const = 10.0 # Initial coefficient value for main loss term that encourages class change\n",
    "arg_b = 9 # No. of updates to the coefficient of the main loss term\n",
    "arg_kappa = 0.2 # Minimum confidence gap between the PNs (changed) class probability and original class' probability\n",
    "arg_beta = 1e-1 # Controls sparsity of the solution (L1 loss)\n",
    "arg_gamma = 100 # Controls how much to adhere to a (optionally trained) auto-encoder\n",
    "my_AE_model = None # Pointer to an auto-encoder\n",
    "arg_alpha = 0.01 # Penalizes L2 norm of the solution\n",
    "arg_threshold = 1. # Automatically turn off features <= arg_threshold if arg_threshold < 1\n",
    "arg_offset = 0.5 # the model assumes classifier trained on data normalized\n",
    "                # in [-arg_offset, arg_offset] range, where arg_offset is 0 or 0.5\n",
    "# Find PN for applicant 1272\n",
    "(adv_pn, delta_pn, info_pn) = explainer.explain_instance(X, arg_mode, my_AE_model, arg_kappa, arg_b,\n",
    "                                                         arg_max_iter, arg_init_const, arg_beta, arg_gamma,\n",
    "                                                            arg_alpha, arg_threshold, arg_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xpn = adv_pn\n",
    "classes = [ class_names[np.argmax(nn.predict_proba(X))], class_names[np.argmax(nn.predict_proba(Xpn))], 'NIL' ]\n",
    "\n",
    "print(\"Sample:\", idx)\n",
    "print(\"prediction(X)\", nn.predict_proba(X), class_names[np.argmax(nn.predict_proba(X))])\n",
    "print(\"prediction(Xpn)\", nn.predict_proba(Xpn), class_names[np.argmax(nn.predict_proba(Xpn))] )\n",
    "\n",
    "\n",
    "X_re = rescale(X) # Convert values back to original scale from normalized\n",
    "Xpn_re = rescale(Xpn)\n",
    "Xpn_re = np.around(Xpn_re.astype(np.double), 2)\n",
    "\n",
    "delta_re = Xpn_re - X_re\n",
    "delta_re = np.around(delta_re.astype(np.double), 2)\n",
    "delta_re[np.absolute(delta_re) < 1e-4] = 0\n",
    "\n",
    "X3 = np.vstack((X_re, Xpn_re, delta_re))\n",
    "\n",
    "dfre = pd.DataFrame.from_records(X3) # Create dataframe to display original point, PN and difference (delta)\n",
    "dfre[23] = classes\n",
    "\n",
    "dfre.columns = df.columns\n",
    "dfre.rename(index={0:'X',1:'X_PN', 2:'(X_PN - X)'}, inplace=True)\n",
    "dfret = dfre.transpose()\n",
    "\n",
    "\n",
    "def highlight_ce(s, col, ncols):\n",
    "    if (type(s[col]) != str):\n",
    "        if (s[col] > 0):\n",
    "            return(['background-color: yellow']*ncols)    \n",
    "    return(['background-color: white']*ncols)\n",
    "\n",
    "dfret.style.apply(highlight_ce, col='(X_PN - X)', ncols=3, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcdefaults()\n",
    "fi = abs((X-Xpn).astype('double'))/np.std(xn_train.astype('double'), axis=0) # Compute PN feature importance\n",
    "objects = df.columns[-2::-1]\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = fi[0, -1::-1]\n",
    "\n",
    "plt.barh(y_pos, performance, align='center', alpha=0.5) # bar chart\n",
    "plt.yticks(y_pos, objects) # Display features on y-axis\n",
    "plt.xlabel('weight') # x-label\n",
    "plt.title('PN (feature importance)') # Heading\n",
    "\n",
    "plt.show() # Display PN feature importance"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
